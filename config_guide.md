# Конфигурационный файл

На примере конфиг-файла [experiments_multimodal/P1_exp2/train.yaml](experiments_multimodal/P1_exp2/train.yaml) 
параметры, которые вероятнее всего надо будет менять:


- `experiment` ('P1_exp2') - имя эксперимента, оно же - имя папки эксперимента

### Параметры, связанные с данными

- `dataset -> train -> data_path` (/disk/sdb1/avatars/dataset_EPE_data1/train) - полный путь к папке с набором данных для обучения
- `dataset -> train -> domain` должен быть `real`
- `dataset -> train -> modalities`: это группа параметров, описывающая варианты входных данных, используемых при обучении.
    - `... -> ground_truth`: тут всегда `name: frames` (должно совпадать с именем папки), `type: standard`, `ext` - по усмотрению, это список возможных расширений изображений `frames`.
    - `... -> input_1`: это первый вариант входных данных. Важно, чтобы он шел следом за ground_truth и имет номер 1. На него накладывается предсказание модели. `name` должно совпадать с именем папки, содержащей соответствующие данные, `type` - тип данных, от этого зависит способ предобработки, и `ext` - список возможных расширений файлов.
    - `... -> input_<X>`: все последующие варианты входных данных описываются тем же образом, что и первый.
    - `... -> input_<X> -> type`: тип предобрабртки данных. Может быть `standart`, `semantic`, `semantic2`, `normal_map`.
    	- `normal_map`: предобработка для изображений с нормалями,
    	- `semantic`: предобработка для сегментационных изображений, содержащих лишь области-маски различных классов, закодированные цветом;
    	- `semantic2`: аналогично `semantic`, но группирует некоторые из классов в одну категорию (например, левый и правый глаз);
    	- `standart`: данные лишь приводятся к тензору (для всех остальных вариантов - оригинальные кадры, с размытием, стилизованные и т.д.).
    - `gt_crop_size`: размер квадратного фрагмента, извлекаемого из входных данных (таков способ обучения). Чем больше - тем больше памяти требует модель.
    - `batch_size`: размер батча
- `dataset -> validate<X>`: аналогично `train`. Можно задать несколько вариантов валидации, отличающихся данными, параметрами: `validate1`, `validate2` и т.д. Все эти варианты будут выполнены на этапе валидации.

### Параметры модели

- `model -> generator -> in_nc`: суммарное число входных каналов, складывается по всем вариантам входа. Изображения с типом `semantic`, `semantic2`, имеют число каналов, равное числу классов в сегментационных масках, все остальные имеют 3 канала. Например, если на вход подаются изображение с размытием, стилизованное изображение и сегментационная маска на 10 классов, то результат будет 3+3+10=16.
- `model -> discriminator -> orig_nc`: суммарное число входных каналов, то же самое число, что и `model -> generator -> in_nc`.
- `model -> generator -> load_path`: путь к pth-файлу с предобученными весами ГЕНЕРАТОРА (если надо дообучить модель или запустить на валидацию/тест/инференс)
- `model -> discriminator -> load_path`: путь к pth-файлу с предобученными весами ДИСКРИМИНАТОРА (если надо дообучить модель)

### Параметры обучения

- `train -> start_iter`: если это дообучение, нужно указать номер итерации, для которого поданы веса
- `train -> total_iter`: до какого числа итераций обучаем
- `train -> res_dir`: путь к папке, куда сохраняются логи обучения - примеры изображений из валидационной базы и т.д. (веса сохраняются в папку эксперимента)

### Параметры валидации

- `test -> test_freq`: раз в сколько итераций выполнять валидацию
- `test -> save_res`: в какую папку сохранять небольшую выборку результатов валидации

### Параметры логирования

- `logger -> log_freq`: раз в сколько итераций сохранять логи (значения метрик)
- `logger -> ckpt_freq`: раз в сколько итераций сохранять веса

